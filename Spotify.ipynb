{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the Spotify Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "# Load the required libraries\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "# rdflib knows about some namespaces, like FOAF\n",
    "from rdflib.namespace import FOAF, XSD\n",
    "# CHECK DATE \n",
    "import datetime\n",
    "from rdflib.namespace import OWL, RDFS\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the country and the spotify ontology namespaces not known by RDFlib\n",
    "CNS = Namespace(\"http://eulersharp.sourceforge.net/2003/03swap/countries#\")\n",
    "SP = Namespace(\"http://www.dei.unipd.it/GraphDatabases/SpotifyOntology#\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters and URLs\n",
    "path = str(Path(os.path.abspath(os.getcwd())).parent.absolute())\n",
    "\n",
    "# #spotify codes\n",
    "spotify = os.path.join(path, 'INDEX', 'spotify', 'dataset', 'chart.csv')\n",
    "\n",
    "# saving folder\n",
    "savePath =  path + '/INDEX/spotify/dataset/rdf/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "spotify_data = pd.read_csv(spotify)\n",
    "\n",
    "# Extract unique regions as countries, ensuring no duplicates\n",
    "Countries = spotify_data[['region']].drop_duplicates().rename(columns={'region': 'country_name'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"sp\", SP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 14 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Initialize a counter for country IDs starting from 1\n",
    "country_counter = 1\n",
    "\n",
    "# Iterate over the unique country names\n",
    "for _, row in Countries.iterrows():\n",
    "    # Create the node to add to the Graph with a sequential ID\n",
    "    Country = URIRef(SP['country' + str(country_counter)])\n",
    "    \n",
    "    # Add triples using store's add() method\n",
    "    g.add((Country, RDF.type, SP.Country))\n",
    "    g.add((Country, SP['region'], Literal(row['country_name'], datatype=XSD.string)))\n",
    "    \n",
    "    # Increment the counter for the next country\n",
    "    country_counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 17.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "# with open(savePath + 'leagues.ttl', 'w') as file:\n",
    "with open(savePath + 'country.ttl', 'w', encoding='utf-8') as file:\n",
    "    file.write(g.serialize(format='turtle'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"so\", SP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique artists from the dataset\n",
    "# Process the artists\n",
    "artists = spotify_data['artist'].unique()  # Get unique artists\n",
    "\n",
    "for index, artist in enumerate(artists):\n",
    "    # Create a unique URI for each artist\n",
    "    artist_id = f\"artist{index + 1}\"  # ID starts from 1\n",
    "    Artist = URIRef(SP[artist_id])\n",
    "    \n",
    "    # Add triples for the artist\n",
    "    g.add((Artist, RDF.type, SP.Artist))  # Declare as an Artist\n",
    "    g.add((Artist, FOAF.name, Literal(artist, datatype=XSD.string)))  # Add name\n",
    "\n",
    "    # Link Artist to Person using isMemberOf\n",
    "    Person = URIRef(SP[f\"person{index + 1}\"])  # Unique URI for Person\n",
    "    g.add((Artist, SP['isMemberOf'], Person))\n",
    "    g.add((Person, RDF.type, FOAF.Person))  # Declare as a Person\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 5.81 s\n",
      "Wall time: 5.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'artist.ttl', 'w', encoding='utf-8') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDF graph\n",
    "g = Graph()\n",
    "g.bind(\"spotify\", SP)\n",
    "g.bind(\"countries\", CNS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to map artist names to URIs\n",
    "artist_mapping = {}\n",
    "\n",
    "# Process the songs\n",
    "for index, row in spotify_data.iterrows():\n",
    "    # Create a unique URI for each song\n",
    "    song_id = f\"song{index + 1}\"  # ID starts from 1\n",
    "    Song = URIRef(SP[song_id])\n",
    "    \n",
    "    # Add triples for the song\n",
    "    g.add((Song, RDF.type, SP.Song))  # Declare as a Song\n",
    "    g.add((Song, SP['songUrl'], Literal(row['url'], datatype=XSD.string)))  # Add song URL\n",
    "    g.add((Song, SP['songName'], Literal(row['title'], datatype=XSD.string)))  \n",
    "    \n",
    "    # Add the relationship \"PerformedBy\" (Song â†’ Artist)\n",
    "    artist_name = row['artist']\n",
    "    \n",
    "    # Check if the artist already exists in the mapping\n",
    "    if artist_name not in artist_mapping:\n",
    "        # Create a new URI for the artist\n",
    "        artist_id = f\"artist{len(artist_mapping) + 1}\"\n",
    "        Artist = URIRef(SP[artist_id])\n",
    "        \n",
    "        # Add artist to the mapping\n",
    "        artist_mapping[artist_name] = Artist\n",
    "        \n",
    "        # Add triples for the artist\n",
    "        g.add((Artist, RDF.type, SP.Artist))\n",
    "        g.add((Artist, FOAF.name, Literal(artist_name, datatype=XSD.string)))  #Use artist_name\n",
    "        g.add((Artist, SP['isMemberOf'], URIRef(\"http://xmlns.com/foaf/0.1/Person\")))  # Link to FOAF Person\n",
    "    \n",
    "    # Link the song to the artist\n",
    "    g.add((Song, SP['PerformedBy'], artist_mapping[artist_name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 2min 28s\n",
      "Wall time: 2min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'song.ttl', 'w', encoding='utf-8') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDF graph\n",
    "g = Graph()\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"spotify\", SP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Chart and its subclasses\n",
    "Chart = URIRef(SP.Chart)\n",
    "Top200 = URIRef(SP.Top200)\n",
    "Viral50 = URIRef(SP.Viral50)\n",
    "\n",
    "# Add triples for Chart and its subclasses\n",
    "g.add((Chart, RDF.type, RDFS.Class))\n",
    "g.add((Top200, RDF.type, RDFS.Class))\n",
    "g.add((Viral50, RDF.type, RDFS.Class))\n",
    "g.add((Top200, RDFS.subClassOf, Chart))\n",
    "g.add((Viral50, RDFS.subClassOf, Chart))\n",
    "\n",
    "\n",
    "\n",
    "# Add publishedIn relationships for Charts\n",
    "for chart_type in ['Top200', 'Viral50']:\n",
    "    chart_instance = URIRef(SP[chart_type])\n",
    "    for region in spotify_data['region'].unique():\n",
    "        # Link each chart type to the existing country URIs (defined in countries.ttl)\n",
    "        country_id = f\"country{spotify_data['region'].unique().tolist().index(region) + 1}\"\n",
    "        Country = URIRef(CNS[country_id])  # Reuse existing country URIs\n",
    "        g.add((chart_instance, SP['publishedIn'], Country))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 20.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'chart.ttl', 'w', encoding='utf-8') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RankedRecorded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_data.rename(columns={\"rank\": \"RankedRecord\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDF graph\n",
    "g = Graph()\n",
    "g.bind(\"spotify\", SP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# Define trendStatus values\n",
    "trend_status_values = {\n",
    "    \"MOVE_UP\": SP.MOVE_UP,\n",
    "    \"MOVE_DOWN\": SP.MOVE_DOWN,\n",
    "    \"SAME_POSITION\": SP.SAME_POSITION,\n",
    "    \"NEW_ENTRY\": SP.NEW_ENTRY,\n",
    "}\n",
    "\n",
    "\n",
    "# Rename the rank column to RankedRecord\n",
    "spotify_data.rename(columns={\"rank\": \"RankedRecord\"}, inplace=True)\n",
    "\n",
    "# Create RDF graph\n",
    "g = Graph()\n",
    "g.bind(\"spotify\", SP)\n",
    "\n",
    "# Process ranks\n",
    "for index, row in spotify_data.iterrows():\n",
    "    # Create a unique URI for each rank\n",
    "    rank_id = f\"rank{index + 1}\"  # Unique ID for each rank\n",
    "    Rank = URIRef(SP[rank_id])\n",
    "    \n",
    "    # Add triples for the rank\n",
    "    g.add((Rank, RDF.type, SP.Rank))  # Declare as a Rank\n",
    "    g.add((Rank, SP['hasRank'], Literal(int(row['RankedRecord']), datatype=XSD.integer)))  # Add rank position\n",
    "    \n",
    "    # Convert the date to ISO 8601 format\n",
    "    try:\n",
    "        # Try converting the date to a proper datetime object and then format it\n",
    "        date_object = datetime.strptime(row['date'], '%m/%d/%Y')  # Adjust the format if needed\n",
    "        formatted_date = date_object.isoformat()  # Convert to ISO 8601 format\n",
    "        g.add((Rank, SP['Date'], Literal(formatted_date, datatype=XSD.dateTime)))  # Add date\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping invalid date: {row['date']}. Error: {e}\")\n",
    "        continue  # Skip rows with invalid dates\n",
    "    \n",
    "    # Check if the song is in the Top200 chart and if stream_count is available\n",
    "    if row['chart'] == 'Top200' and pd.notna(row['streams']):\n",
    "        g.add((Rank, SP['stream_count'], Literal(int(row['streams']), datatype=XSD.integer)))  # Add stream count\n",
    "\n",
    "    # Add trendStatus\n",
    "    trend_status = trend_status_values.get(row['trend'], None)\n",
    "    if trend_status:\n",
    "        g.add((Rank, SP['trendStatus'], trend_status))  # Add trend status\n",
    "    \n",
    "    # Add associatedWithChart relationship (Top200 or Viral50)\n",
    "    chart_type = row['chart'].lower()  # Assume values are \"top200\" or \"viral50\"\n",
    "    if chart_type == \"top200\":\n",
    "        Chart = URIRef(SP.Top200)\n",
    "    elif chart_type == \"viral50\":\n",
    "        Chart = URIRef(SP.Viral50)\n",
    "    else:\n",
    "        continue\n",
    "    g.add((Rank, SP['associatedWithChart'], Chart))  # Link rank to chart\n",
    "\n",
    "    # Add AssignToSong relationship\n",
    "    song_id = f\"song{index + 1}\"  # Assume Song URIs follow similar indexing\n",
    "    Song = URIRef(SP[song_id])\n",
    "    g.add((Rank, SP['AssignToSong'], Song))  # Link rank to song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 1min 52s\n",
      "Wall time: 1min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'RankedRecorded.ttl', 'w', encoding='utf-8') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
